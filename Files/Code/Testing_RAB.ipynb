{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac92f81",
   "metadata": {},
   "source": [
    "# Instalando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8f4704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.3.2 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: matplotlib==3.10.6 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: seaborn==0.13.2 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn==1.7.1 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy==2.2.6 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: pyarrow==21.0.0 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (21.0.0)\n",
      "Requirement already satisfied: torch==2.8.0 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\range\\appdata\\roaming\\python\\python313\\site-packages (from pandas==2.3.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas==2.3.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas==2.3.2) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib==3.10.6) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib==3.10.6) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib==3.10.6) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib==3.10.6) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\range\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib==3.10.6) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib==3.10.6) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib==3.10.6) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn==1.7.1) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn==1.7.1) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn==1.7.1) (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0) (80.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\range\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas==2.3.2) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\range\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch==2.8.0) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Installing libraries \"\"\"\n",
    "%pip install pandas==2.3.2 matplotlib==3.10.6 seaborn==0.13.2 scikit-learn==1.7.1 numpy==2.2.6 pyarrow==21.0.0 torch==2.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b8257",
   "metadata": {},
   "source": [
    "# Importando bibliotecas (externas e próprias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566e1485",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Modules.models.make_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mModules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloading\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mread_parquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m read_parquet_file \n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mModules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01monehot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m one_hot_encode_parquet\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mModules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmake_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SingleSeriesDataset, MultiSeriesDataset\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mModules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mNBeats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NBeatsBlock, NBeats\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mModules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mWMAPELoss\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WMAPELoss\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'Modules.models.make_dataset'"
     ]
    }
   ],
   "source": [
    "\"\"\" Importing libraries \"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path so 'Modules' can be imported\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Our modules\n",
    "from Modules.loading.read_parquet import read_parquet_file \n",
    "from Modules.preprocessing.onehot import one_hot_encode_parquet\n",
    "from Modules.models.make_dataset import SingleSeriesDataset, MultiSeriesDataset\n",
    "from Modules.models.NBeats import NBeatsBlock, NBeats\n",
    "from Modules.models.WMAPELoss import WMAPELoss\n",
    "from Modules.models.training import train_model\n",
    "from Modules.models.test import soft_test\n",
    "from Modules.models.test import hard_test\n",
    "from Modules.models.test import hard_test1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d7650c",
   "metadata": {},
   "source": [
    "# Definição dos hiper-parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4bbbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Defining hyper-parameters \"\"\"\n",
    "# Neural Network Global Parameters\n",
    "input_size = 7  # Number of past days to use as input\n",
    "output_size = 1  # Number of future days to predict\n",
    "batch_size = 14  # Batch size for training\n",
    "\n",
    "n_layers = 4  # Number of layers in the N-BEATS model\n",
    "hidden_size = 32  # Number of hidden units in each layer\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 1e-3 # Learning rate for the optimizer\n",
    "epochs = 1000  # Number of training epochs (iterations over the entire dataset)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f16a3",
   "metadata": {},
   "source": [
    "# Importação do Dataset\n",
    "\n",
    "É interessante dividir o treino em batches (mini-conjuntos de treino). Cada batch possui o tamanho de input size, seguindo a ordem cronológica de vendas dentro daquela janela de dias. No entanto, durante o treinamento é **ESSENCIAL** que a escolha do próximo batch seja aleatória.\n",
    "\n",
    "Ex.: Inicia o treino por 21-27 jul e prevê 28, depois pula para 02-08 fev para prever 03. Esse processo deve ser repetido até todos os dados serem treinados, finalizando **01 epoch**.\n",
    "\n",
    "O número de **epochs** diz o número total de iterações do modelo com relação ao dataset inteiro.\n",
    "\n",
    "Sobre a composição da janela de input dentro de um batch, existem duas abordagens:\n",
    "\n",
    "1) Treinar em cada janela todas as séries (pense que cada par produto-loja x tempo representa uma série temporal dentro daquele período). Esse modelo é bem mais complexo pois o output deve ter o mesmo tamanho de produto-loja.\n",
    "2) Treinar vários modelos separados (considerando uma série temporal para cada modelo). Esse método é ineficiente pois o modelo nunca irá aprender os padrões entre as séries.\n",
    "3) Treinar o modelo com um par produto-loja por vez. Ou seja:\n",
    "   - O modelo realiza epochs = N iterações de treino ao longo de todo dataset\n",
    "     - Em cada epoch, passa por todas as M batches\n",
    "       - Em cada batch (que possui uma janela de tamanho input_size), atualiza os parâmetros para cada série temporal ($x_l,y_l$). Totalizando L atualizações, com L sendo o número de pares produto-loja.\n",
    "\n",
    "Ressalta-se que cada conjunto ($x_l,y_l$) representa:\n",
    "- $x_l$: série temporal do l-ésimo par produto-loja, sendo um vetor de tamanho input_size x (features + 1)\n",
    "- $y_l$: Previsão de vendas do l-ésimo par produto-loja para os próximos $output_size$ dias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebc9357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Setting up the constants \"\"\"\n",
    "\n",
    "DATA_PATH = \"../Data/hackathon_2025_templates/part-00000-tid-\"\n",
    "\n",
    "FILE_HASH = [\n",
    "    \"2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36\",\n",
    "    \"5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8\",\n",
    "    \"7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1\",\n",
    "]\n",
    "\n",
    "SUFIX = \"-4-1-c000.snappy.parquet\"\n",
    "\n",
    "FILE_NAMES = [DATA_PATH + hash + SUFIX for hash in FILE_HASH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87170c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loading the data \"\"\"\n",
    "\n",
    "pdv_data = read_parquet_file(FILE_NAMES[0])\n",
    "transaction_data = read_parquet_file(FILE_NAMES[1])\n",
    "product_data = read_parquet_file(FILE_NAMES[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d896b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcacadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f4e83",
   "metadata": {},
   "source": [
    "# Reorganizando os dados em pares produto-loja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) juntar produto\n",
    "df = transaction_data.merge(\n",
    "    product_data,\n",
    "    left_on=\"internal_product_id\",\n",
    "    right_on=\"produto\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 2) juntar loja\n",
    "df = df.merge(\n",
    "    pdv_data,\n",
    "    left_on=\"internal_store_id\",\n",
    "    right_on=\"pdv\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 3) garantir que a data esteja no formato datetime\n",
    "df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"])\n",
    "\n",
    "# 4) pivotar: dia x par produto-loja\n",
    "full_df = (\n",
    "    df.pivot_table(\n",
    "        index=\"transaction_date\",                        # agora usa transaction_date\n",
    "        columns=[\"internal_product_id\", \"internal_store_id\"],  \n",
    "        values=\"quantity\",                               # mantenha 'quantity' se esse for o nome certo\n",
    "        aggfunc=\"sum\",\n",
    "        fill_value=0\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5) deixar colunas mais legíveis: produto_loja\n",
    "full_df.columns = [f\"{p}_{l}\" for p, l in full_df.columns]\n",
    "\n",
    "# 6) garantir todas as datas do período (mesmo sem vendas)\n",
    "full_df = full_df.reindex(\n",
    "    pd.date_range(df[\"transaction_date\"].min(), df[\"transaction_date\"].max(), freq=\"D\"),\n",
    "    fill_value=0\n",
    ")\n",
    "full_df.index.name = \"data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f71577",
   "metadata": {},
   "source": [
    "# Transformando o Dataframe em um Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação de um dataset sintético e bem comportado\n",
    "# (apenas para testar o modelo, já que o real está muito bagunçado)\n",
    "\n",
    "n_data = 1000 # Número total de pontos de dados\n",
    "\n",
    "# Sinal composto por várias frequências + ruído\n",
    "time = np.linspace(0, 20 * np.pi, n_data) # Tempo de 0 a 20π\n",
    "\n",
    "series_1 = np.sin(time) + 0.5 * np.sin(3 * time) + 0.2 * np.sin(5 * time) + 0.1 * np.random.randn(n_data)\n",
    "\n",
    "series_2 = np.cos(time) + 0.3 * np.sin(2 * time) + 0.4 * np.sin(4 * time) + 0.15 * np.random.randn(n_data)\n",
    "\n",
    "# Cálculo da feature como a diferença entre o valor atual e o anterior (característica cíclica!!!)\n",
    "feature_1 = np.array([0] +[series_1[t] - series_1[t-1] for t in range(1, len(series_1))])\n",
    "feature_2 = np.array([0] + [series_2[t] - series_2[t-1] for t in range(1, len(series_2))])\n",
    "\n",
    "# Normalização dos dados para a faixa [0, 1] - MÉTODO SIMPLES, PESQUISAR OUTROS\n",
    "series_1 = (series_1 - np.min(series_1)) / (np.max(series_1) - np.min(series_1))\n",
    "feature_1 = (feature_1 - np.min(feature_1)) / (np.max(feature_1) - np.min(feature_1))\n",
    "\n",
    "series_2 = (series_2 - np.min(series_2)) / (np.max(series_2) - np.min(series_2))\n",
    "feature_2 = (feature_2 - np.min(feature_2)) / (np.max(feature_2) - np.min(feature_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf39687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando os vetores de entrada e os rótulos\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Número total de amostras (janelas diferentes) que podem ser extraídas\n",
    "n_samples = len(series_1) - input_size - output_size + 1\n",
    "\n",
    "# Extraindo janelas deslizantes\n",
    "for i in range(n_samples):\n",
    "\n",
    "    X_window1 = series_1[i:i+input_size]             # janela de entrada\n",
    "    X_window2 = series_2[i:i+input_size]             # janela de entrada\n",
    "    feature_1window = feature_1[i:i+input_size]      # janela de entrada\n",
    "    feature_2window = feature_2[i:i+input_size]      # janela de entrada\n",
    "\n",
    "    X_window = np.stack([X_window1, X_window2, feature_1window, feature_2window], axis=1)  # shape = (input_size, 4)\n",
    "\n",
    "    y_window1 = series_1[i+input_size:i+input_size+output_size]  # próximo dia da série 1\n",
    "    y_window2 = series_2[i+input_size:i+input_size+output_size]  # próximo dia da série 2\n",
    "\n",
    "    # Agora o rótulo também tem 2 valores (para cada série)\n",
    "    y_window = np.stack([y_window1, y_window2], axis=1)  # shape = (output_size, 2)\n",
    "    X.append(X_window)\n",
    "    y.append(y_window)\n",
    "\n",
    "# Convertendo para arrays numpy e depois para tensores PyTorch\n",
    "X = np.array(X)  # shape = [n_samples, input_size]\n",
    "y = np.array(y)  # shape = [n_samples, output_size]\n",
    "\n",
    "print(X.shape)\n",
    "X = torch.tensor(X, dtype=torch.float32)  # shape = [n_samples, input_size, 1]\n",
    "\n",
    "y = torch.tensor(y, dtype=torch.float32)  # shape = [n_samples, 1]\n",
    "\n",
    "\" Definindo o Dataset e DataLoader do PyTorch \"\n",
    "class SingleSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset_full = SingleSeriesDataset(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1109f2",
   "metadata": {},
   "source": [
    "# Separando os dados entre treino / validação\n",
    "\n",
    "Os dados serão separados na proporção 80% - treino / 20% validação. Para séries temporais, é usual que essa separação seja feita de forma cronológica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbf17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponto de separação entre treino e validação\n",
    "split_point = int(0.8 * len(dataset_full))\n",
    "\n",
    "# Separação cronológica das janelas\n",
    "X_train, X_test = X[:split_point], X[split_point:]\n",
    "y_train, y_test = y[:split_point], y[split_point:]\n",
    "\n",
    "num_features = X_train.shape[2]  # número de features (2 no caso: valor e tempo)\n",
    "# Flatten do tensor para entrar na rede\n",
    "X_train = X_train.view(X_train.shape[0], -1)  # shape = [n_samples, input_size * n_features]\n",
    "X_test  = X_test.view(X_test.shape[0], -1)\n",
    "\n",
    "print(f\"Há um total de {len(dataset_full)} janelas e o split_point = {split_point}, em 80%\")\n",
    "print(f\" O shape de X_train é {X_train.shape} e o shape de X_test é {X_test.shape}\")\n",
    "print(f\" O shape de y_train é {y_train.shape} e o shape de y_test é {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9145c17",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c15ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adotando o dataset de treino\n",
    "dataset = SingleSeriesDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=False) # shuffle=False para séries tempor\n",
    "\n",
    "# Inicialização do modelo N-BEATS (considerando X_train com múltiplas features)\n",
    "model = NBeats(input_size * num_features, hidden_size, output_size, n_layers).to(device)\n",
    "\n",
    "model, criterion, optimizer = train_model(model, learning_rate, epochs, device, dataloader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c436ce4",
   "metadata": {},
   "source": [
    "# Validação do modelo\n",
    "\n",
    "São feitos dois testes:\n",
    "\n",
    "- **Soft test:** Modelo  tenta fazer as previsões, mas não utiliza-as nas previsões futuras, utiliza sempre os *ground truth* como input\n",
    "- **Hard test:** Modelo tenta fazer as previsões, e utiliza $y_{i-1}$ para a previsão de $y_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8561181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_S, all_targets_S, avg_loss_test_S = soft_test(model, dataloader, device, criterion)\n",
    "\n",
    "# Hard test está dando erro, conferir depois\n",
    "all_preds_H, all_targets_H, avg_loss_test_H = hard_test(model, dataloader, device, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf5b083",
   "metadata": {},
   "source": [
    "Apesar da diferença grande entre os erros do **Hard Test** e **Soft Test**, ressalta-se que, ao adicionar features exógenas aos dados não previstos (dia da semana, do mês, mês) essas features auxiliam na estabilidade do input, mesmo que o próprio modelo continue fornecendo informações imperfeitas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cee228b",
   "metadata": {},
   "source": [
    "# Ponto de parada - RAB 18-09-25\n",
    "\n",
    "Nosso modelo está acertando bem nos dados de treino e de teste fácil, mas (não se sabe se) ele performa mal no treino difícil, pois o erro se acumula rápido. Isso é um indicativo de:\n",
    "- Se quisermos mandar bem em dados desconhecidos completamente, precisaremos de muitas features auxiliares\n",
    "- O dataset de exemplo é bem comportado. Queremos então séries bem comportadas (mesmo que sejam ruidosas, mas presentes) para nossa rede. Valores de outliers podem ser:\n",
    "  - Descartados\n",
    "  - Previstos probabilisticamente\n",
    "  - De forma trivial, falar que, para todos os outliers de jan-22, se repetirá em jan-23\n",
    "\n",
    "No entanto, para isso precisamos entender melhor os dados, o que eu sugiro:\n",
    "1) Pegar a tabelona de produto-loja e tentar filtrar aquelas que são relevantes\n",
    "2) Pegar, para cada loja, os k-ésimos (1, 3, 5) produtos mais vendidos ao longo do ano, ou mais frequentes, e visualizar esses sinais\n",
    "3) Ver, no geral, quais produtos são mais populares dentre todo dataset\n",
    "\n",
    "Após fazermos essas análises, teremos uma boa ideia de que parte do nosso dataset será boa para inputar a caixa preta. Após essa escolha, pensamos nas features:\n",
    "- Temporais (mais fáceis de pensar e implementar)\n",
    "- Categóricas (intrínsecas do produto ou da loja, como a categoria deles)\n",
    "- De localização (apenas se der tempo)\n",
    "\n",
    "Após termos feito isso, podemos então realizar normalização dos sinais, one hot encoding do que for viável e embedding de IDs etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9865c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683c9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
