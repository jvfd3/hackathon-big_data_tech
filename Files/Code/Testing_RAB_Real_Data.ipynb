{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac92f81",
   "metadata": {},
   "source": [
    "# Instalando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8f4704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Installing libraries \"\"\"\n",
    "%pip install --quiet pandas==2.3.2 matplotlib==3.10.6 seaborn==0.13.2 scikit-learn==1.7.1 numpy==2.2.6 pyarrow==21.0.0\n",
    "%pip install --quiet torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b8257",
   "metadata": {},
   "source": [
    "# Importando bibliotecas (externas e próprias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566e1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Importing libraries \"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path so 'Modules' can be imported\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Our modules\n",
    "from Modules.evaluation.output_formatter import forecast_to_output\n",
    "from Modules.loading.plug_n_play import get_clean_data\n",
    "from Modules.loading.read_parquet import read_parquet_file \n",
    "from Modules.models.forecast import forecast_blind\n",
    "from Modules.models.make_dataset import SingleSeriesDataset, MultiSeriesDataset\n",
    "from Modules.models.NBeats import NBeatsBlock, NBeats\n",
    "from Modules.models.test import hard_test\n",
    "from Modules.models.test import soft_test\n",
    "from Modules.models.training import train_model\n",
    "from Modules.models.WMAPELoss import WMAPELoss\n",
    "from Modules.preprocessing.onehot import one_hot_encode_parquet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d7650c",
   "metadata": {},
   "source": [
    "# Definição dos hiper-parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a4bbbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Defining hyper-parameters \"\"\"\n",
    "\n",
    "HYPERPARAMS = {\n",
    "    # Neural Network Global Parameters\n",
    "    'input_size': 30,       # Number of past days to use as input\n",
    "    'output_size': 7,       # Number of future days to predict\n",
    "    'batch_size': 28,       # Batch size for training\n",
    "    'n_layers': 4,          # Number of layers in the N-BEATS model\n",
    "    'hidden_size': 128,     # Number of hidden units in each layer\n",
    "\n",
    "    # Training parameters\n",
    "    'learning_rate': 1e-3,  # Learning rate for the optimizer\n",
    "    'epochs': 100,          # Number of training epochs (iterations over the entire dataset)\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),  # Use GPU if available\n",
    "    'blind_horizon': 4,     # Number of days to exclude from the end of the training set for hard test\n",
    "    'split': 1,             # Proportion of data to use for training (1.0 for validation)\n",
    "    'seed': 42,             # Random seed for reproducibility\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f16a3",
   "metadata": {},
   "source": [
    "# Importação do Dataset\n",
    "\n",
    "É interessante dividir o treino em batches (mini-conjuntos de treino). Cada batch possui o tamanho de input size, seguindo a ordem cronológica de vendas dentro daquela janela de dias. No entanto, durante o treinamento é **ESSENCIAL** que a escolha do próximo batch seja aleatória.\n",
    "\n",
    "Ex.: Inicia o treino por 21-27 jul e prevê 28, depois pula para 02-08 fev para prever 03. Esse processo deve ser repetido até todos os dados serem treinados, finalizando **01 epoch**.\n",
    "\n",
    "O número de **epochs** diz o número total de iterações do modelo com relação ao dataset inteiro.\n",
    "\n",
    "Sobre a composição da janela de input dentro de um batch, existem duas abordagens:\n",
    "\n",
    "1) Treinar em cada janela todas as séries (pense que cada par produto-loja x tempo representa uma série temporal dentro daquele período). Esse modelo é bem mais complexo pois o output deve ter o mesmo tamanho de produto-loja.\n",
    "2) Treinar vários modelos separados (considerando uma série temporal para cada modelo). Esse método é ineficiente pois o modelo nunca irá aprender os padrões entre as séries.\n",
    "3) Treinar o modelo com um par produto-loja por vez. Ou seja:\n",
    "   - O modelo realiza epochs = N iterações de treino ao longo de todo dataset\n",
    "     - Em cada epoch, passa por todas as M batches\n",
    "       - Em cada batch (que possui uma janela de tamanho input_size), atualiza os parâmetros para cada série temporal ($x_l,y_l$). Totalizando L atualizações, com L sendo o número de pares produto-loja.\n",
    "\n",
    "Ressalta-se que cada conjunto ($x_l,y_l$) representa:\n",
    "- $x_l$: série temporal do l-ésimo par produto-loja, sendo um vetor de tamanho input_size x (features + 1)\n",
    "- $y_l$: Previsão de vendas do l-ésimo par produto-loja para os próximos $output_size$ dias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6388577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:202 FILE_PATHS\n",
      "00:00:000 loaded_data\n",
      "00:03:379 numerical_table\n",
      "00:01:144 outlierless\n",
      "00:01:138 pivoted_df\n",
      "00:24:699 rescaled_df\n",
      "00:17:918 copying\n",
      "00:01:219 returning\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Recebe os dados filtrados / limpos \"\"\"\n",
    "\n",
    "clean_data = get_clean_data(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa2a1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" get dataset function \"\"\"\n",
    "\n",
    "def get_dataset(sample, feature, hyperparams):\n",
    "    input_size = hyperparams['input_size']\n",
    "    output_size = hyperparams['output_size']\n",
    "\n",
    "    # Inicializando os vetores de entrada e os rótulos\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Número total de amostras (janelas diferentes) que podem ser extraídas\n",
    "    # de um mesmo sample\n",
    "    num_windows = len(sample) - input_size - output_size + 1\n",
    "    print(f\"Número total de janelas extraídas: {num_windows}\")\n",
    "    print(f\"Para janelas de tamanho {input_size} e previsão de {output_size} dias à frente.\")\n",
    "    # Extraindo janelas deslizantes\n",
    "    for i in range(num_windows):\n",
    "\n",
    "        X_window = sample[i:i+input_size]             # janela de entrada\n",
    "        feature_window = feature[i:i+input_size]      # janela de entrada\n",
    "\n",
    "        X_window = np.stack([X_window,feature_window], axis=1)  # shape = (input_size, 4)\n",
    "\n",
    "        y_window = sample[i+input_size:i+input_size+output_size]  # próximos dias da série\n",
    "\n",
    "        X.append(X_window)\n",
    "        y.append(y_window)\n",
    "\n",
    "    # Convertendo para arrays numpy e depois para tensores PyTorch\n",
    "    X = np.array(X)  # shape = [num_windows, input_size, num_features]\n",
    "    y = np.array(y)  # shape = [num_windows, output_size]\n",
    "    print()\n",
    "    print(\"O vetor de entrada  antes do flatten\")\n",
    "    print(f\"tem shape (num_windows, size_window, num_features): {X.shape}\")\n",
    "\n",
    "    X = torch.tensor(X, dtype=torch.float32)  # shape = [n_samples, input_size, 1]\n",
    "\n",
    "    y = torch.tensor(y, dtype=torch.float32)  # shape = [n_samples, 1]\n",
    "\n",
    "\n",
    "    dataset_full = SingleSeriesDataset(X, y)\n",
    "    return dataset_full, X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa342bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Training Model \"\"\"\n",
    "\n",
    "def traininig_func(X_train, y_train, num_features, hyperparams):\n",
    "    # Adotando o dataset de treino\n",
    "    batch_size = hyperparams['batch_size']\n",
    "    input_size = hyperparams['input_size']\n",
    "    hidden_size = hyperparams['hidden_size']\n",
    "    output_size = hyperparams['output_size']\n",
    "    n_layers = hyperparams['n_layers']\n",
    "    device = hyperparams['device']\n",
    "    learning_rate = hyperparams['learning_rate']\n",
    "    epochs = hyperparams['epochs']\n",
    "    \n",
    "    num_features = num_features\n",
    "    \n",
    "    dataset = SingleSeriesDataset(X_train, y_train) \n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=False) # shuffle=False para séries temporais\n",
    "\n",
    "    # Inicialização do modelo N-BEATS (considerando X_train com num_features)\n",
    "    model = NBeats(input_size*num_features, hidden_size, output_size, n_layers).to(device)\n",
    "    model, criterion, optimizer = train_model(model, learning_rate, epochs, device, dataloader)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2c68833",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Training Model \"\"\"\n",
    "\n",
    "def get_blind_prediction(model, X_train, hyperparams):\n",
    "    blind_horizon = hyperparams['blind_horizon']\n",
    "    device = hyperparams['device']\n",
    "    output_size = hyperparams['output_size']\n",
    "    split = hyperparams['split']\n",
    "\n",
    "    if split != 1.0:\n",
    "        print(\"Previsão cega não realizada, pois split < 1.0\")\n",
    "        return\n",
    "\n",
    "    blind_prediction = forecast_blind(model, X_train, blind_horizon, device, output_size)\n",
    "    return blind_prediction\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6d7f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Separando treino e validação \"\"\"\n",
    "\n",
    "def get_train_validation(dataset_full, X, y, hyperparams):\n",
    "    split = hyperparams['split']\n",
    "\n",
    "    X_test, y_test = None, None\n",
    "    X_train, y_train = None, None\n",
    "    \n",
    "    # Ponto de separação entre treino e validação (Caso seja para envio, não há validação)\n",
    "    if split < 1:\n",
    "        split_point = int(split * len(dataset_full))\n",
    "        # Separação cronológica das janelas\n",
    "        X_train, X_test = X[:split_point], X[split_point:]\n",
    "        y_train, y_test = y[:split_point], y[split_point:]\n",
    "    else:\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "\n",
    "    num_features = X_train.shape[2]\n",
    "    \n",
    "    # Flatten do tensor para entrar na rede\n",
    "    X_train = X_train.view(X_train.shape[0], -1)  # shape = [num_windows_train, input_size * n_features]\n",
    "\n",
    "    if split < 1:\n",
    "        X_test  = X_test.view(X_test.shape[0], -1) # shape = [num_windows_test, input_size * n_features]\n",
    "\n",
    "    print(f\"Há um total de {len(dataset_full)} janelas e o split ocorre em {split*100:.0f}% do dataset\")\n",
    "    print(f\" O shape de X_train é {X_train.shape} e o shape de X_test é {X_test.shape}\") if split < 1 else \"\"\n",
    "    print(f\" O shape de y_train é {y_train.shape} e o shape de y_test é {y_test.shape}\") if split < 1 else \"\"\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, num_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a06621cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Get Sample \"\"\"\n",
    "\n",
    "def get_sample(clean_data, col):\n",
    "    sample = clean_data[col].values\n",
    "    # sample = clean_data[sampled_col.sum()].values\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(clean_data)), sample)\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "607da96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create Feature and Rescale \"\"\"\n",
    "\n",
    "def create_feature_rescale(sampled): # Rescaling e sampling\n",
    "    sample = (sampled - np.min(sampled)) / (np.max(sampled) - np.min(sampled))\n",
    "    \n",
    "    feature = [sampled[t] - sampled[t-1] for t in range(1, len(sampled))]\n",
    "    feature = (feature - np.min(feature)) / (np.max(feature) - np.min(feature))\n",
    "    return sample, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87fdface",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_predictions(clean_data, prediction_col, hyperparams):\n",
    "    sample = get_sample(clean_data, prediction_col)\n",
    "    sample, feature = create_feature_rescale(sample)\n",
    "    dataset_full, X, y = get_dataset(sample, feature, hyperparams)\n",
    "    X_train, y_train, X_test, y_test, num_features = get_train_validation(dataset_full, X, y, hyperparams)\n",
    "    model = traininig_func(X_train, y_train, num_features, hyperparams)\n",
    "    blind_prediction = get_blind_prediction(model, X_train, hyperparams)\n",
    "    return blind_prediction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "153eaa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Get dataframe predictions \"\"\"\n",
    "def get_dataframe_predictions(clean_data, HYPERPARAMS):\n",
    "    final_predictions = dict()\n",
    "\n",
    "    for col in clean_data[:5]:\n",
    "        final_predictions[col] = get_predictions(clean_data, col, HYPERPARAMS)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622ce60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de janelas extraídas: 329\n",
      "Para janelas de tamanho 30 e previsão de 7 dias à frente.\n",
      "\n",
      "O vetor de entrada  antes do flatten\n",
      "tem shape (num_windows, size_window, num_features): (329, 30, 2)\n",
      "Há um total de 329 janelas e o split ocorre em 100% do dataset\n",
      "Epoch 1/100, Loss: 522392402.9838\n",
      "Epoch 2/100, Loss: 213790258.2588\n",
      "Epoch 3/100, Loss: 117154210.8618\n",
      "Epoch 4/100, Loss: 86218116.8562\n",
      "Epoch 5/100, Loss: 78751121.5168\n",
      "Epoch 6/100, Loss: 111352649.8630\n",
      "Epoch 7/100, Loss: 97748298.2090\n",
      "Epoch 8/100, Loss: 94805119.1916\n",
      "Epoch 9/100, Loss: 94329078.5347\n",
      "Epoch 10/100, Loss: 92464550.5509\n",
      "Epoch 11/100, Loss: 73285918.8484\n",
      "Epoch 12/100, Loss: 82713816.8662\n",
      "Epoch 13/100, Loss: 69639330.8535\n",
      "Epoch 14/100, Loss: 81307077.1964\n",
      "Epoch 15/100, Loss: 77465369.1967\n",
      "Epoch 16/100, Loss: 68715701.1810\n",
      "Epoch 17/100, Loss: 59515641.8523\n",
      "Epoch 18/100, Loss: 43293903.8452\n",
      "Epoch 19/100, Loss: 49760650.1875\n",
      "Epoch 20/100, Loss: 45240857.3447\n",
      "Epoch 21/100, Loss: 43133660.5101\n",
      "Epoch 22/100, Loss: 54583632.5114\n",
      "Epoch 23/100, Loss: 50764851.0192\n",
      "Epoch 24/100, Loss: 57558531.1775\n",
      "Epoch 25/100, Loss: 54964940.1829\n",
      "Epoch 26/100, Loss: 50462311.5191\n",
      "Epoch 27/100, Loss: 48844771.1753\n",
      "Epoch 28/100, Loss: 43313464.1745\n",
      "Epoch 29/100, Loss: 49794552.8444\n",
      "Epoch 30/100, Loss: 48052619.1768\n",
      "Epoch 31/100, Loss: 37147185.3452\n",
      "Epoch 32/100, Loss: 38157251.8416\n",
      "Epoch 33/100, Loss: 51574601.5197\n",
      "Epoch 34/100, Loss: 59368921.1851\n",
      "Epoch 35/100, Loss: 79323246.8543\n",
      "Epoch 36/100, Loss: 61622062.8553\n",
      "Epoch 37/100, Loss: 39761780.1741\n",
      "Epoch 38/100, Loss: 38031548.8466\n",
      "Epoch 39/100, Loss: 58818111.1865\n",
      "Epoch 40/100, Loss: 76719325.8633\n",
      "Epoch 41/100, Loss: 57868583.1935\n",
      "Epoch 42/100, Loss: 44639119.5127\n",
      "Epoch 43/100, Loss: 44800283.5117\n",
      "Epoch 44/100, Loss: 58485320.5131\n",
      "Epoch 45/100, Loss: 55310522.8567\n",
      "Epoch 46/100, Loss: 52964836.5217\n",
      "Epoch 47/100, Loss: 66170763.1816\n",
      "Epoch 48/100, Loss: 63066713.8583\n",
      "Epoch 49/100, Loss: 57474986.5220\n",
      "Epoch 50/100, Loss: 59757485.1814\n",
      "Epoch 51/100, Loss: 47868104.8518\n",
      "Epoch 52/100, Loss: 43087678.6798\n",
      "Epoch 53/100, Loss: 68502570.8528\n",
      "Epoch 54/100, Loss: 56204759.1957\n",
      "Epoch 55/100, Loss: 68060718.5097\n",
      "Epoch 56/100, Loss: 73841465.8611\n",
      "Epoch 57/100, Loss: 53653726.5291\n",
      "Epoch 58/100, Loss: 68501715.5154\n",
      "Epoch 59/100, Loss: 57999891.5186\n",
      "Epoch 60/100, Loss: 55451943.8533\n",
      "Epoch 61/100, Loss: 53859972.5156\n",
      "Epoch 62/100, Loss: 48505726.1822\n",
      "Epoch 63/100, Loss: 55097189.1837\n",
      "Epoch 64/100, Loss: 45975840.5121\n",
      "Epoch 65/100, Loss: 45766956.1852\n",
      "Epoch 66/100, Loss: 43533010.1747\n",
      "Epoch 67/100, Loss: 46610579.3454\n",
      "Epoch 68/100, Loss: 46634779.8449\n",
      "Epoch 69/100, Loss: 35989527.5115\n",
      "Epoch 70/100, Loss: 40592094.8506\n",
      "Epoch 71/100, Loss: 55842596.6794\n",
      "Epoch 72/100, Loss: 43061563.8530\n",
      "Epoch 73/100, Loss: 40620856.3431\n",
      "Epoch 74/100, Loss: 30802931.5072\n",
      "Epoch 75/100, Loss: 33840442.6748\n",
      "Epoch 76/100, Loss: 47088816.8416\n",
      "Epoch 77/100, Loss: 45021084.1823\n",
      "Epoch 78/100, Loss: 40233977.8435\n",
      "Epoch 79/100, Loss: 31055827.8446\n",
      "Epoch 80/100, Loss: 31945598.8420\n",
      "Epoch 81/100, Loss: 47171506.6788\n",
      "Epoch 82/100, Loss: 49340243.8491\n",
      "Epoch 83/100, Loss: 45582526.1811\n",
      "Epoch 84/100, Loss: 65278427.5131\n",
      "Epoch 85/100, Loss: 48585246.0244\n",
      "Epoch 86/100, Loss: 38701488.1795\n",
      "Epoch 87/100, Loss: 43665759.5193\n",
      "Epoch 88/100, Loss: 44821671.6775\n",
      "Epoch 89/100, Loss: 33571203.8481\n",
      "Epoch 90/100, Loss: 39749831.5087\n",
      "Epoch 91/100, Loss: 35012562.6805\n",
      "Epoch 92/100, Loss: 37802028.8445\n",
      "Epoch 93/100, Loss: 47314439.8419\n",
      "Epoch 94/100, Loss: 32885583.1743\n",
      "Epoch 95/100, Loss: 33757979.0060\n",
      "Epoch 96/100, Loss: 31214153.5123\n",
      "Epoch 97/100, Loss: 31205411.5061\n",
      "Epoch 98/100, Loss: 30878290.4239\n",
      "Epoch 99/100, Loss: 34658424.0124\n",
      "Epoch 100/100, Loss: 25196230.0880\n",
      "Número total de janelas extraídas: 329\n",
      "Para janelas de tamanho 30 e previsão de 7 dias à frente.\n",
      "\n",
      "O vetor de entrada  antes do flatten\n",
      "tem shape (num_windows, size_window, num_features): (329, 30, 2)\n",
      "Há um total de 329 janelas e o split ocorre em 100% do dataset\n",
      "Epoch 1/100, Loss: 560342181.4683\n",
      "Epoch 2/100, Loss: 199976298.7804\n",
      "Epoch 3/100, Loss: 102928974.0935\n",
      "Epoch 4/100, Loss: 81483452.0971\n",
      "Epoch 5/100, Loss: 76002689.4229\n",
      "Epoch 6/100, Loss: 85507317.0937\n",
      "Epoch 7/100, Loss: 96786272.0940\n",
      "Epoch 8/100, Loss: 69997037.0920\n",
      "Epoch 9/100, Loss: 54615986.5911\n",
      "Epoch 10/100, Loss: 49743110.4208\n",
      "Epoch 11/100, Loss: 67371505.7567\n",
      "Epoch 12/100, Loss: 51869189.7563\n",
      "Epoch 13/100, Loss: 65961359.7553\n",
      "Epoch 14/100, Loss: 52126525.2602\n",
      "Epoch 15/100, Loss: 37874982.5865\n",
      "Epoch 16/100, Loss: 75051556.4232\n",
      "Epoch 17/100, Loss: 88309700.0927\n",
      "Epoch 18/100, Loss: 76260753.7567\n",
      "Epoch 19/100, Loss: 71060249.7572\n",
      "Epoch 20/100, Loss: 48771967.4229\n",
      "Epoch 21/100, Loss: 60812492.7554\n",
      "Epoch 22/100, Loss: 50524800.2569\n",
      "Epoch 23/100, Loss: 46961597.0885\n",
      "Epoch 24/100, Loss: 78835348.9251\n",
      "Epoch 25/100, Loss: 69770237.7604\n",
      "Epoch 26/100, Loss: 77654585.7570\n",
      "Epoch 27/100, Loss: 82779650.0965\n",
      "Epoch 28/100, Loss: 53627604.4219\n",
      "Epoch 29/100, Loss: 43730222.0888\n",
      "Epoch 30/100, Loss: 37942594.4192\n",
      "Epoch 31/100, Loss: 39183407.5878\n",
      "Epoch 32/100, Loss: 42171297.7534\n",
      "Epoch 33/100, Loss: 45934465.5894\n",
      "Epoch 34/100, Loss: 54442758.4222\n",
      "Epoch 35/100, Loss: 28557831.4205\n",
      "Epoch 36/100, Loss: 26820079.6691\n",
      "Epoch 37/100, Loss: 46769756.2540\n",
      "Epoch 38/100, Loss: 58925433.0907\n",
      "Epoch 39/100, Loss: 57827340.4223\n",
      "Epoch 40/100, Loss: 50984771.0892\n",
      "Epoch 41/100, Loss: 42204627.5881\n",
      "Epoch 42/100, Loss: 51050933.2578\n",
      "Epoch 43/100, Loss: 70885620.4245\n",
      "Epoch 44/100, Loss: 51583691.9214\n",
      "Epoch 45/100, Loss: 55576682.4235\n",
      "Epoch 46/100, Loss: 40331878.9220\n",
      "Epoch 47/100, Loss: 37402325.5873\n",
      "Epoch 48/100, Loss: 40448299.7537\n",
      "Epoch 49/100, Loss: 46982405.2554\n",
      "Epoch 50/100, Loss: 43839833.7562\n",
      "Epoch 51/100, Loss: 34031680.7528\n",
      "Epoch 52/100, Loss: 26597355.7535\n",
      "Epoch 53/100, Loss: 28581961.4197\n",
      "Epoch 54/100, Loss: 35992833.2533\n",
      "Epoch 55/100, Loss: 41472103.7579\n",
      "Epoch 56/100, Loss: 42762153.7543\n",
      "Epoch 57/100, Loss: 41520433.5888\n",
      "Epoch 58/100, Loss: 37352539.5883\n",
      "Epoch 59/100, Loss: 31620437.2530\n",
      "Epoch 60/100, Loss: 39498343.4209\n",
      "Epoch 61/100, Loss: 39745128.7555\n",
      "Epoch 62/100, Loss: 38118191.5874\n",
      "Epoch 63/100, Loss: 39125160.8386\n",
      "Epoch 64/100, Loss: 32562925.3350\n",
      "Epoch 65/100, Loss: 24576884.0866\n",
      "Epoch 66/100, Loss: 33402606.7540\n",
      "Epoch 67/100, Loss: 42455278.4217\n",
      "Epoch 68/100, Loss: 43566767.0886\n",
      "Epoch 69/100, Loss: 44101251.4236\n",
      "Epoch 70/100, Loss: 51859798.4227\n",
      "Epoch 71/100, Loss: 48372967.5875\n",
      "Epoch 72/100, Loss: 43034370.0912\n",
      "Epoch 73/100, Loss: 35846137.0858\n",
      "Epoch 74/100, Loss: 37286628.7553\n",
      "Epoch 75/100, Loss: 30156695.0053\n",
      "Epoch 76/100, Loss: 40418701.5865\n",
      "Epoch 77/100, Loss: 35597129.2556\n",
      "Epoch 78/100, Loss: 39592849.2540\n",
      "Epoch 79/100, Loss: 30400903.5872\n",
      "Epoch 80/100, Loss: 25141369.4197\n",
      "Epoch 81/100, Loss: 38283249.4198\n",
      "Epoch 82/100, Loss: 33091844.0877\n",
      "Epoch 83/100, Loss: 27995706.8365\n",
      "Epoch 84/100, Loss: 26215381.0029\n",
      "Epoch 85/100, Loss: 27839774.9199\n",
      "Epoch 86/100, Loss: 45770732.7534\n",
      "Epoch 87/100, Loss: 47700629.5888\n",
      "Epoch 88/100, Loss: 31530309.4195\n",
      "Epoch 89/100, Loss: 27199068.6691\n",
      "Epoch 90/100, Loss: 36976299.5869\n",
      "Epoch 91/100, Loss: 34777792.4194\n",
      "Epoch 92/100, Loss: 32486377.4219\n",
      "Epoch 93/100, Loss: 27873730.7524\n",
      "Epoch 94/100, Loss: 37192164.5860\n",
      "Epoch 95/100, Loss: 47041368.7556\n",
      "Epoch 96/100, Loss: 26661412.9190\n",
      "Epoch 97/100, Loss: 19002476.5025\n",
      "Epoch 98/100, Loss: 28611862.9192\n",
      "Epoch 99/100, Loss: 38264152.9204\n",
      "Epoch 100/100, Loss: 31765812.7535\n",
      "Número total de janelas extraídas: 329\n",
      "Para janelas de tamanho 30 e previsão de 7 dias à frente.\n",
      "\n",
      "O vetor de entrada  antes do flatten\n",
      "tem shape (num_windows, size_window, num_features): (329, 30, 2)\n",
      "Há um total de 329 janelas e o split ocorre em 100% do dataset\n",
      "Epoch 1/100, Loss: 334220768.1041\n",
      "Epoch 2/100, Loss: 123969838.4304\n",
      "Epoch 3/100, Loss: 65690899.7617\n",
      "Epoch 4/100, Loss: 70356404.7582\n",
      "Epoch 5/100, Loss: 57592230.4243\n",
      "Epoch 6/100, Loss: 61330187.0889\n",
      "Epoch 7/100, Loss: 39726638.4223\n",
      "Epoch 8/100, Loss: 48092512.7568\n",
      "Epoch 9/100, Loss: 66986752.0926\n",
      "Epoch 10/100, Loss: 57108213.7592\n",
      "Epoch 11/100, Loss: 59137975.7570\n",
      "Epoch 12/100, Loss: 50546074.0901\n",
      "Epoch 13/100, Loss: 59257090.0909\n",
      "Epoch 14/100, Loss: 66683878.7569\n",
      "Epoch 15/100, Loss: 56977166.0904\n",
      "Epoch 16/100, Loss: 66138774.4234\n",
      "Epoch 17/100, Loss: 58703979.9231\n",
      "Epoch 18/100, Loss: 38057423.9221\n",
      "Epoch 19/100, Loss: 37444539.5873\n",
      "Epoch 20/100, Loss: 39255528.4220\n",
      "Epoch 21/100, Loss: 51309455.0061\n",
      "Epoch 22/100, Loss: 53583106.5895\n",
      "Epoch 23/100, Loss: 40976229.4216\n",
      "Epoch 24/100, Loss: 52455930.0896\n",
      "Epoch 25/100, Loss: 59673859.4229\n",
      "Epoch 26/100, Loss: 56670056.7564\n",
      "Epoch 27/100, Loss: 52585840.2579\n",
      "Epoch 28/100, Loss: 50011973.2574\n",
      "Epoch 29/100, Loss: 51471842.4250\n",
      "Epoch 30/100, Loss: 60024317.0914\n",
      "Epoch 31/100, Loss: 53087801.0891\n",
      "Epoch 32/100, Loss: 47225498.0880\n",
      "Epoch 33/100, Loss: 51735427.9234\n",
      "Epoch 34/100, Loss: 41338160.2548\n",
      "Epoch 35/100, Loss: 48090545.4221\n",
      "Epoch 36/100, Loss: 40535367.5872\n",
      "Epoch 37/100, Loss: 45975822.0907\n",
      "Epoch 38/100, Loss: 51451848.2557\n",
      "Epoch 39/100, Loss: 32479693.7537\n",
      "Epoch 40/100, Loss: 29727144.9210\n",
      "Epoch 41/100, Loss: 41291822.7538\n",
      "Epoch 42/100, Loss: 43905777.9217\n",
      "Epoch 43/100, Loss: 41993192.5882\n",
      "Epoch 44/100, Loss: 46637862.5917\n",
      "Epoch 45/100, Loss: 49729027.4251\n",
      "Epoch 46/100, Loss: 47348148.4230\n",
      "Epoch 47/100, Loss: 42793535.5901\n",
      "Epoch 48/100, Loss: 53155796.5928\n",
      "Epoch 49/100, Loss: 52133891.4227\n",
      "Epoch 50/100, Loss: 51501712.7579\n",
      "Epoch 51/100, Loss: 49476595.9234\n",
      "Epoch 52/100, Loss: 54338827.5902\n",
      "Epoch 53/100, Loss: 62542528.0907\n",
      "Epoch 54/100, Loss: 45132049.0875\n",
      "Epoch 55/100, Loss: 40750832.0866\n",
      "Epoch 56/100, Loss: 43803882.7554\n",
      "Epoch 57/100, Loss: 48414997.5890\n",
      "Epoch 58/100, Loss: 50601180.7559\n",
      "Epoch 59/100, Loss: 46693160.5901\n",
      "Epoch 60/100, Loss: 42775527.4217\n",
      "Epoch 61/100, Loss: 41067653.0879\n",
      "Epoch 62/100, Loss: 36937851.2537\n",
      "Epoch 63/100, Loss: 33396602.7546\n",
      "Epoch 64/100, Loss: 37383293.4215\n",
      "Epoch 65/100, Loss: 35725743.5877\n",
      "Epoch 66/100, Loss: 35476882.7535\n",
      "Epoch 67/100, Loss: 34119010.7540\n",
      "Epoch 68/100, Loss: 31392673.9206\n",
      "Epoch 69/100, Loss: 29894580.0872\n",
      "Epoch 70/100, Loss: 28983044.4191\n",
      "Epoch 71/100, Loss: 31474606.4199\n",
      "Epoch 72/100, Loss: 23827795.7533\n",
      "Epoch 73/100, Loss: 24213469.5877\n",
      "Epoch 74/100, Loss: 31169772.7548\n",
      "Epoch 75/100, Loss: 33447807.0042\n",
      "Epoch 76/100, Loss: 47826766.5885\n",
      "Epoch 77/100, Loss: 53182778.7573\n",
      "Epoch 78/100, Loss: 44317045.4228\n",
      "Epoch 79/100, Loss: 53453379.4241\n",
      "Epoch 80/100, Loss: 51234372.5920\n",
      "Epoch 81/100, Loss: 53947946.9237\n",
      "Epoch 82/100, Loss: 48509441.4215\n",
      "Epoch 83/100, Loss: 53619236.9257\n",
      "Epoch 84/100, Loss: 31616670.4199\n",
      "Epoch 85/100, Loss: 29793398.7538\n",
      "Epoch 86/100, Loss: 42862301.0897\n",
      "Epoch 87/100, Loss: 32488322.4195\n",
      "Epoch 88/100, Loss: 37519202.7551\n",
      "Epoch 89/100, Loss: 28908424.7535\n",
      "Epoch 90/100, Loss: 22866078.1693\n",
      "Epoch 91/100, Loss: 35066335.8403\n",
      "Epoch 92/100, Loss: 36619876.5888\n",
      "Epoch 93/100, Loss: 41839501.9226\n",
      "Epoch 94/100, Loss: 42375872.5875\n",
      "Epoch 95/100, Loss: 42585073.7553\n",
      "Epoch 96/100, Loss: 35794417.5876\n",
      "Epoch 97/100, Loss: 27495563.0879\n",
      "Epoch 98/100, Loss: 42416782.7572\n",
      "Epoch 99/100, Loss: 35742601.0877\n",
      "Epoch 100/100, Loss: 39094496.4215\n",
      "Número total de janelas extraídas: 329\n",
      "Para janelas de tamanho 30 e previsão de 7 dias à frente.\n",
      "\n",
      "O vetor de entrada  antes do flatten\n",
      "tem shape (num_windows, size_window, num_features): (329, 30, 2)\n",
      "Há um total de 329 janelas e o split ocorre em 100% do dataset\n",
      "Epoch 1/100, Loss: 742830381.4667\n",
      "Epoch 2/100, Loss: 227967396.7703\n",
      "Epoch 3/100, Loss: 100307301.4313\n",
      "Epoch 4/100, Loss: 110554055.4317\n",
      "Epoch 5/100, Loss: 117871590.7597\n",
      "Epoch 6/100, Loss: 77639695.7596\n",
      "Epoch 7/100, Loss: 67933653.5921\n",
      "Epoch 8/100, Loss: 48960536.7549\n",
      "Epoch 9/100, Loss: 75545297.0928\n",
      "Epoch 10/100, Loss: 109873322.1008\n",
      "Epoch 11/100, Loss: 84404893.7640\n",
      "Epoch 12/100, Loss: 87578630.4277\n",
      "Epoch 13/100, Loss: 54262492.4200\n",
      "Epoch 14/100, Loss: 46201199.7556\n",
      "Epoch 15/100, Loss: 63252868.7580\n",
      "Epoch 16/100, Loss: 86221102.4271\n",
      "Epoch 17/100, Loss: 95191056.4280\n",
      "Epoch 18/100, Loss: 65252490.4233\n",
      "Epoch 19/100, Loss: 51765539.4230\n",
      "Epoch 20/100, Loss: 55693133.4220\n",
      "Epoch 21/100, Loss: 44289541.4229\n",
      "Epoch 22/100, Loss: 45294092.2552\n",
      "Epoch 23/100, Loss: 52015714.4225\n",
      "Epoch 24/100, Loss: 49552902.7566\n",
      "Epoch 25/100, Loss: 56421260.4254\n",
      "Epoch 26/100, Loss: 43234702.2541\n",
      "Epoch 27/100, Loss: 44972480.0892\n",
      "Epoch 28/100, Loss: 54528229.7558\n",
      "Epoch 29/100, Loss: 49586261.7568\n",
      "Epoch 30/100, Loss: 48970116.9226\n",
      "Epoch 31/100, Loss: 60679387.4237\n",
      "Epoch 32/100, Loss: 49278838.9245\n",
      "Epoch 33/100, Loss: 57176650.5921\n",
      "Epoch 34/100, Loss: 48894751.0875\n",
      "Epoch 35/100, Loss: 54563584.4250\n",
      "Epoch 36/100, Loss: 50660500.0894\n",
      "Epoch 37/100, Loss: 52031051.0913\n",
      "Epoch 38/100, Loss: 52254596.5875\n",
      "Epoch 39/100, Loss: 39237538.9213\n",
      "Epoch 40/100, Loss: 42683271.4223\n",
      "Epoch 41/100, Loss: 56129959.4229\n",
      "Epoch 42/100, Loss: 50936673.5891\n",
      "Epoch 43/100, Loss: 56254239.9246\n",
      "Epoch 44/100, Loss: 62071630.0906\n",
      "Epoch 45/100, Loss: 55739817.0902\n",
      "Epoch 46/100, Loss: 47849888.5892\n",
      "Epoch 47/100, Loss: 50414443.4231\n",
      "Epoch 48/100, Loss: 41922657.5043\n",
      "Epoch 49/100, Loss: 50652354.0908\n",
      "Epoch 50/100, Loss: 53400418.9265\n",
      "Epoch 51/100, Loss: 45495595.4208\n",
      "Epoch 52/100, Loss: 41698253.5887\n",
      "Epoch 53/100, Loss: 48269216.0873\n",
      "Epoch 54/100, Loss: 44517670.9200\n",
      "Epoch 55/100, Loss: 43761619.2551\n",
      "Epoch 56/100, Loss: 46406465.5877\n",
      "Epoch 57/100, Loss: 55801468.9223\n",
      "Epoch 58/100, Loss: 33618836.5883\n",
      "Epoch 59/100, Loss: 39708192.2547\n",
      "Epoch 60/100, Loss: 36266918.7544\n",
      "Epoch 61/100, Loss: 56325215.4235\n",
      "Epoch 62/100, Loss: 37410374.0882\n",
      "Epoch 63/100, Loss: 53187036.7581\n",
      "Epoch 64/100, Loss: 49132569.2566\n",
      "Epoch 65/100, Loss: 43112357.4229\n",
      "Epoch 66/100, Loss: 38142249.7539\n",
      "Epoch 67/100, Loss: 49215597.9229\n",
      "Epoch 68/100, Loss: 42866143.0890\n",
      "Epoch 69/100, Loss: 44885299.9217\n",
      "Epoch 70/100, Loss: 41606230.7541\n",
      "Epoch 71/100, Loss: 43630712.4229\n",
      "Epoch 72/100, Loss: 42329544.2551\n",
      "Epoch 73/100, Loss: 37809155.2541\n",
      "Epoch 74/100, Loss: 40947109.7548\n",
      "Epoch 75/100, Loss: 33128159.0884\n",
      "Epoch 76/100, Loss: 35772294.0873\n",
      "Epoch 77/100, Loss: 47125532.5896\n",
      "Epoch 78/100, Loss: 35435778.0875\n",
      "Epoch 79/100, Loss: 36262409.0884\n",
      "Epoch 80/100, Loss: 39798262.0880\n",
      "Epoch 81/100, Loss: 40948064.2558\n",
      "Epoch 82/100, Loss: 38229370.5889\n",
      "Epoch 83/100, Loss: 30557637.9203\n",
      "Epoch 84/100, Loss: 22171538.2528\n",
      "Epoch 85/100, Loss: 33223657.4208\n",
      "Epoch 86/100, Loss: 36968828.2532\n",
      "Epoch 87/100, Loss: 33770211.7535\n",
      "Epoch 88/100, Loss: 39929741.9221\n",
      "Epoch 89/100, Loss: 43180496.9211\n",
      "Epoch 90/100, Loss: 42606887.7539\n",
      "Epoch 91/100, Loss: 41115345.0886\n",
      "Epoch 92/100, Loss: 41545397.4232\n",
      "Epoch 93/100, Loss: 37982798.7551\n",
      "Epoch 94/100, Loss: 25517141.0017\n",
      "Epoch 95/100, Loss: 22467499.8367\n",
      "Epoch 96/100, Loss: 33716156.9215\n",
      "Epoch 97/100, Loss: 39180275.2539\n",
      "Epoch 98/100, Loss: 30362098.9205\n",
      "Epoch 99/100, Loss: 26248378.0869\n",
      "Epoch 100/100, Loss: 37974816.4217\n",
      "Número total de janelas extraídas: 329\n",
      "Para janelas de tamanho 30 e previsão de 7 dias à frente.\n",
      "\n",
      "O vetor de entrada  antes do flatten\n",
      "tem shape (num_windows, size_window, num_features): (329, 30, 2)\n",
      "Há um total de 329 janelas e o split ocorre em 100% do dataset\n",
      "Epoch 1/100, Loss: 352515340.3577\n",
      "Epoch 2/100, Loss: 168162816.2353\n",
      "Epoch 3/100, Loss: 125666928.2348\n",
      "Epoch 4/100, Loss: 126381838.8881\n",
      "Epoch 5/100, Loss: 83732592.5363\n",
      "Epoch 6/100, Loss: 86029565.8871\n",
      "Epoch 7/100, Loss: 59179923.8608\n",
      "Epoch 8/100, Loss: 77042770.8801\n",
      "Epoch 9/100, Loss: 98097294.9011\n",
      "Epoch 10/100, Loss: 107894318.8924\n",
      "Epoch 11/100, Loss: 99503750.2271\n",
      "Epoch 12/100, Loss: 92634706.8774\n",
      "Epoch 13/100, Loss: 84458407.2254\n",
      "Epoch 14/100, Loss: 101930609.5485\n",
      "Epoch 15/100, Loss: 93483078.5526\n",
      "Epoch 16/100, Loss: 69461082.5470\n",
      "Epoch 17/100, Loss: 81903556.8770\n",
      "Epoch 18/100, Loss: 51739458.5278\n",
      "Epoch 19/100, Loss: 85729650.5609\n",
      "Epoch 20/100, Loss: 64292876.2058\n",
      "Epoch 21/100, Loss: 62560898.2060\n",
      "Epoch 22/100, Loss: 62557447.8630\n",
      "Epoch 23/100, Loss: 69478467.5210\n",
      "Epoch 24/100, Loss: 69222263.0317\n",
      "Epoch 25/100, Loss: 89527184.2104\n",
      "Epoch 26/100, Loss: 77337731.2103\n",
      "Epoch 27/100, Loss: 57047325.5357\n",
      "Epoch 28/100, Loss: 47663157.8599\n",
      "Epoch 29/100, Loss: 85693675.5419\n",
      "Epoch 30/100, Loss: 77329848.5291\n",
      "Epoch 31/100, Loss: 59372772.0342\n",
      "Epoch 32/100, Loss: 67553533.5458\n",
      "Epoch 33/100, Loss: 54902453.3477\n",
      "Epoch 34/100, Loss: 45430898.1859\n",
      "Epoch 35/100, Loss: 59157286.8577\n",
      "Epoch 36/100, Loss: 78238367.2125\n",
      "Epoch 37/100, Loss: 75215834.5415\n",
      "Epoch 38/100, Loss: 65121289.2035\n",
      "Epoch 39/100, Loss: 63268505.2046\n",
      "Epoch 40/100, Loss: 44563528.5136\n",
      "Epoch 41/100, Loss: 57072375.5294\n",
      "Epoch 42/100, Loss: 66956796.3708\n",
      "Epoch 43/100, Loss: 45369608.8552\n",
      "Epoch 44/100, Loss: 38301144.8562\n",
      "Epoch 45/100, Loss: 35184017.3551\n",
      "Epoch 46/100, Loss: 34506255.3504\n",
      "Epoch 47/100, Loss: 31678778.6913\n",
      "Epoch 48/100, Loss: 43756559.5200\n",
      "Epoch 49/100, Loss: 37553335.6860\n",
      "Epoch 50/100, Loss: 42205084.5244\n",
      "Epoch 51/100, Loss: 54580453.5318\n",
      "Epoch 52/100, Loss: 65520180.1995\n",
      "Epoch 53/100, Loss: 58632252.8584\n",
      "Epoch 54/100, Loss: 40709094.1842\n",
      "Epoch 55/100, Loss: 40811401.8529\n",
      "Epoch 56/100, Loss: 39843372.5194\n",
      "Epoch 57/100, Loss: 51648238.8596\n",
      "Epoch 58/100, Loss: 57277669.5440\n",
      "Epoch 59/100, Loss: 42998548.5183\n",
      "Epoch 60/100, Loss: 51523536.1922\n",
      "Epoch 61/100, Loss: 62226264.5291\n",
      "Epoch 62/100, Loss: 62197712.5310\n",
      "Epoch 63/100, Loss: 43579360.3585\n",
      "Epoch 64/100, Loss: 43164775.3603\n",
      "Epoch 65/100, Loss: 46261537.8637\n",
      "Epoch 66/100, Loss: 57351332.1993\n",
      "Epoch 67/100, Loss: 32595396.0170\n",
      "Epoch 68/100, Loss: 29085287.0155\n",
      "Epoch 69/100, Loss: 39836304.1904\n",
      "Epoch 70/100, Loss: 43527794.8465\n",
      "Epoch 71/100, Loss: 50033256.8686\n",
      "Epoch 72/100, Loss: 49393222.5224\n",
      "Epoch 73/100, Loss: 41130819.5267\n",
      "Epoch 74/100, Loss: 42030144.5280\n",
      "Epoch 75/100, Loss: 47154505.1818\n",
      "Epoch 76/100, Loss: 43010619.8489\n",
      "Epoch 77/100, Loss: 44215881.8546\n",
      "Epoch 78/100, Loss: 37948861.8534\n",
      "Epoch 79/100, Loss: 47620710.5246\n",
      "Epoch 80/100, Loss: 50455149.5271\n",
      "Epoch 81/100, Loss: 48165928.5261\n",
      "Epoch 82/100, Loss: 51421520.8626\n",
      "Epoch 83/100, Loss: 44771301.1950\n",
      "Epoch 84/100, Loss: 45641928.5248\n",
      "Epoch 85/100, Loss: 42629711.3563\n",
      "Epoch 86/100, Loss: 46422978.8582\n",
      "Epoch 87/100, Loss: 53508527.8713\n",
      "Epoch 88/100, Loss: 41889596.1905\n",
      "Epoch 89/100, Loss: 42710698.8605\n",
      "Epoch 90/100, Loss: 38505061.6878\n",
      "Epoch 91/100, Loss: 41070982.0255\n",
      "Epoch 92/100, Loss: 34902540.1913\n",
      "Epoch 93/100, Loss: 40383042.5226\n",
      "Epoch 94/100, Loss: 23649479.9326\n",
      "Epoch 95/100, Loss: 28453463.8506\n",
      "Epoch 96/100, Loss: 31973138.6888\n",
      "Epoch 97/100, Loss: 34050542.1826\n",
      "Epoch 98/100, Loss: 27198455.1833\n",
      "Epoch 99/100, Loss: 27249577.5221\n",
      "Epoch 100/100, Loss: 32339216.1805\n",
      "Número total de janelas extraídas: 329\n",
      "Para janelas de tamanho 30 e previsão de 7 dias à frente.\n",
      "\n",
      "O vetor de entrada  antes do flatten\n",
      "tem shape (num_windows, size_window, num_features): (329, 30, 2)\n",
      "Há um total de 329 janelas e o split ocorre em 100% do dataset\n",
      "Epoch 1/100, Loss: 502707384.1308\n",
      "Epoch 2/100, Loss: 209771368.1056\n",
      "Epoch 3/100, Loss: 110949294.7586\n",
      "Epoch 4/100, Loss: 90211933.0908\n",
      "Epoch 5/100, Loss: 68991716.0917\n",
      "Epoch 6/100, Loss: 69858917.4262\n",
      "Epoch 7/100, Loss: 74033951.0959\n",
      "Epoch 8/100, Loss: 107334174.7590\n",
      "Epoch 9/100, Loss: 84242032.0963\n",
      "Epoch 10/100, Loss: 102890017.0954\n",
      "Epoch 11/100, Loss: 78511504.7576\n",
      "Epoch 12/100, Loss: 56967133.7575\n",
      "Epoch 13/100, Loss: 42137427.0873\n",
      "Epoch 14/100, Loss: 62603558.2603\n",
      "Epoch 15/100, Loss: 79877578.7598\n",
      "Epoch 16/100, Loss: 92534346.7565\n",
      "Epoch 17/100, Loss: 78242336.7595\n",
      "Epoch 18/100, Loss: 61922390.7580\n",
      "Epoch 19/100, Loss: 58243032.0925\n",
      "Epoch 20/100, Loss: 53556681.4250\n",
      "Epoch 21/100, Loss: 59778753.0875\n",
      "Epoch 22/100, Loss: 47691346.6751\n",
      "Epoch 23/100, Loss: 57039793.7572\n",
      "Epoch 24/100, Loss: 35921677.0865\n",
      "Epoch 25/100, Loss: 48028949.7561\n",
      "Epoch 26/100, Loss: 67578707.4282\n",
      "Epoch 27/100, Loss: 66862242.4251\n",
      "Epoch 28/100, Loss: 55783374.4237\n",
      "Epoch 29/100, Loss: 41716449.0879\n",
      "Epoch 30/100, Loss: 37233236.0887\n",
      "Epoch 31/100, Loss: 48038972.0910\n",
      "Epoch 32/100, Loss: 47220347.2572\n",
      "Epoch 33/100, Loss: 57100311.9211\n",
      "Epoch 34/100, Loss: 53256633.7563\n",
      "Epoch 35/100, Loss: 57770656.4250\n",
      "Epoch 36/100, Loss: 79346317.7594\n",
      "Epoch 37/100, Loss: 55411315.4241\n",
      "Epoch 38/100, Loss: 68431910.4242\n",
      "Epoch 39/100, Loss: 46207900.2578\n",
      "Epoch 40/100, Loss: 45205877.9206\n",
      "Epoch 41/100, Loss: 40656710.9214\n",
      "Epoch 42/100, Loss: 49889947.4238\n",
      "Epoch 43/100, Loss: 52139077.4231\n",
      "Epoch 44/100, Loss: 70112930.0896\n",
      "Epoch 45/100, Loss: 41106842.0901\n",
      "Epoch 46/100, Loss: 40968877.7538\n",
      "Epoch 47/100, Loss: 50715221.0903\n",
      "Epoch 48/100, Loss: 46386618.0894\n",
      "Epoch 49/100, Loss: 60171772.4236\n",
      "Epoch 50/100, Loss: 42357359.5874\n",
      "Epoch 51/100, Loss: 46129544.4233\n",
      "Epoch 52/100, Loss: 42271814.9215\n",
      "Epoch 53/100, Loss: 55600508.7554\n",
      "Epoch 54/100, Loss: 44189408.2539\n",
      "Epoch 55/100, Loss: 33551528.0886\n",
      "Epoch 56/100, Loss: 44685012.2557\n",
      "Epoch 57/100, Loss: 58186850.5882\n",
      "Epoch 58/100, Loss: 53229132.0907\n",
      "Epoch 59/100, Loss: 42166311.0885\n",
      "Epoch 60/100, Loss: 48650366.4224\n",
      "Epoch 61/100, Loss: 42890586.4245\n",
      "Epoch 62/100, Loss: 45359726.9225\n",
      "Epoch 63/100, Loss: 49756897.7548\n",
      "Epoch 64/100, Loss: 38333975.4223\n",
      "Epoch 65/100, Loss: 50260007.4219\n",
      "Epoch 66/100, Loss: 54056385.5897\n",
      "Epoch 67/100, Loss: 58862360.7559\n",
      "Epoch 68/100, Loss: 42428429.2564\n",
      "Epoch 69/100, Loss: 32344234.2528\n",
      "Epoch 70/100, Loss: 34818532.4196\n",
      "Epoch 71/100, Loss: 37728404.4207\n",
      "Epoch 72/100, Loss: 42667085.9219\n",
      "Epoch 73/100, Loss: 43496830.7564\n",
      "Epoch 74/100, Loss: 40492065.2555\n",
      "Epoch 75/100, Loss: 35877621.2536\n",
      "Epoch 76/100, Loss: 39728813.9229\n",
      "Epoch 77/100, Loss: 52227885.0886\n",
      "Epoch 78/100, Loss: 36066836.0867\n",
      "Epoch 79/100, Loss: 36390861.2540\n",
      "Epoch 80/100, Loss: 28485185.5874\n",
      "Epoch 81/100, Loss: 25789706.4207\n",
      "Epoch 82/100, Loss: 43040318.0894\n",
      "Epoch 83/100, Loss: 50787862.0896\n",
      "Epoch 84/100, Loss: 39841750.7550\n",
      "Epoch 85/100, Loss: 51326845.0897\n",
      "Epoch 86/100, Loss: 42155951.0880\n",
      "Epoch 87/100, Loss: 35506592.9208\n",
      "Epoch 88/100, Loss: 25120048.7535\n",
      "Epoch 89/100, Loss: 41584283.7561\n",
      "Epoch 90/100, Loss: 44344717.7531\n",
      "Epoch 91/100, Loss: 41777793.5864\n",
      "Epoch 92/100, Loss: 39190133.0889\n",
      "Epoch 93/100, Loss: 34448876.2526\n",
      "Epoch 94/100, Loss: 23619533.4202\n",
      "Epoch 95/100, Loss: 37778794.2546\n",
      "Epoch 96/100, Loss: 38934206.7557\n",
      "Epoch 97/100, Loss: 28822795.6700\n",
      "Epoch 98/100, Loss: 38136277.7519\n",
      "Epoch 99/100, Loss: 33984314.9206\n",
      "Epoch 100/100, Loss: 29758575.5856\n",
      "Número total de janelas extraídas: 329\n",
      "Para janelas de tamanho 30 e previsão de 7 dias à frente.\n",
      "\n",
      "O vetor de entrada  antes do flatten\n",
      "tem shape (num_windows, size_window, num_features): (329, 30, 2)\n",
      "Há um total de 329 janelas e o split ocorre em 100% do dataset\n",
      "Epoch 1/100, Loss: 501654420.1264\n",
      "Epoch 2/100, Loss: 162022526.7675\n",
      "Epoch 3/100, Loss: 105885294.4290\n",
      "Epoch 4/100, Loss: 82263434.7591\n",
      "Epoch 5/100, Loss: 75663533.4215\n",
      "Epoch 6/100, Loss: 63718209.0917\n",
      "Epoch 7/100, Loss: 71562411.4250\n",
      "Epoch 8/100, Loss: 86884303.7589\n",
      "Epoch 9/100, Loss: 92230042.0954\n",
      "Epoch 10/100, Loss: 66881747.7596\n",
      "Epoch 11/100, Loss: 73246379.7575\n",
      "Epoch 12/100, Loss: 65426615.7589\n",
      "Epoch 13/100, Loss: 58948059.4226\n",
      "Epoch 14/100, Loss: 61553356.4241\n",
      "Epoch 15/100, Loss: 78694450.4250\n",
      "Epoch 16/100, Loss: 60500451.9239\n",
      "Epoch 17/100, Loss: 56229261.0895\n",
      "Epoch 18/100, Loss: 49744383.7546\n",
      "Epoch 19/100, Loss: 65776047.7576\n",
      "Epoch 20/100, Loss: 84639263.4254\n",
      "Epoch 21/100, Loss: 81957947.0907\n",
      "Epoch 22/100, Loss: 58860913.7563\n",
      "Epoch 23/100, Loss: 60255609.7579\n",
      "Epoch 24/100, Loss: 59986691.4237\n",
      "Epoch 25/100, Loss: 72076919.4268\n",
      "Epoch 26/100, Loss: 50599021.7567\n",
      "Epoch 27/100, Loss: 75986342.7581\n",
      "Epoch 28/100, Loss: 70369160.0912\n",
      "Epoch 29/100, Loss: 61272057.4244\n",
      "Epoch 30/100, Loss: 74867895.7588\n",
      "Epoch 31/100, Loss: 82153058.0932\n",
      "Epoch 32/100, Loss: 67628747.7592\n",
      "Epoch 33/100, Loss: 51596755.7537\n",
      "Epoch 34/100, Loss: 61856452.4250\n",
      "Epoch 35/100, Loss: 54455471.0896\n",
      "Epoch 36/100, Loss: 55417582.4221\n",
      "Epoch 37/100, Loss: 49239388.0909\n",
      "Epoch 38/100, Loss: 48876014.4234\n",
      "Epoch 39/100, Loss: 49797870.4212\n",
      "Epoch 40/100, Loss: 44695781.7543\n",
      "Epoch 41/100, Loss: 42763413.0880\n",
      "Epoch 42/100, Loss: 46347948.4223\n",
      "Epoch 43/100, Loss: 39926870.9208\n",
      "Epoch 44/100, Loss: 36709340.0880\n",
      "Epoch 45/100, Loss: 23340139.0028\n",
      "Epoch 46/100, Loss: 30255609.2546\n",
      "Epoch 47/100, Loss: 48191921.0886\n",
      "Epoch 48/100, Loss: 61009496.0904\n",
      "Epoch 49/100, Loss: 41729076.5880\n",
      "Epoch 50/100, Loss: 40658156.9229\n",
      "Epoch 51/100, Loss: 46934057.7554\n",
      "Epoch 52/100, Loss: 58409662.4230\n",
      "Epoch 53/100, Loss: 47303036.7550\n",
      "Epoch 54/100, Loss: 64725142.4242\n",
      "Epoch 55/100, Loss: 59366994.7562\n",
      "Epoch 56/100, Loss: 59831646.4223\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Modularizando o código de predição \"\"\"\n",
    "\n",
    "final_predictions = get_dataframe_predictions(clean_data, HYPERPARAMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64605f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Output Formatting \"\"\"\n",
    "\n",
    "final_df = forecast_to_output(final_predictions, prediction_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b56034",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afabfb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Sampling and plotting - OK\"\"\"\n",
    "\n",
    "sample = get_sample(clean_data, prediction_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f71577",
   "metadata": {},
   "source": [
    "# Transformando o Dataframe em um Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Rescaling e sampling - OK \"\"\"\n",
    "\n",
    "sample, feature = create_feature_rescale(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf39687",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" get dataset \"\"\"\n",
    "\n",
    "dataset_full, X, y = get_dataset(sample, feature, hyperparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1109f2",
   "metadata": {},
   "source": [
    "# Separando os dados entre treino / validação\n",
    "\n",
    "Os dados serão separados na proporção 80% - treino / 20% validação. Para séries temporais, é usual que essa separação seja feita de forma cronológica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbf17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Separando treino e validação \"\"\"\n",
    "\n",
    "X_train, y_train, X_test, y_test, num_features = get_train_validation(dataset_full, X, y, hyperparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9145c17",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c15ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "\n",
    "model = traininig_func(X_train, y_train, num_features, hyperparams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986ff61",
   "metadata": {},
   "source": [
    "# Sanity check do modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a945c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Sanity Check - TO-DO\"\"\"\n",
    "\n",
    "# sanity_check_plot()\n",
    "\n",
    "# ===\n",
    "\n",
    "def sanity_check_plot(model, dataloader, device, output_size):\n",
    "    \"\"\"\n",
    "    Plota previsões vs ground truth no período de treino\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            preds = model(x)\n",
    "            \n",
    "            # Se seu modelo retorna (batch, output_size)\n",
    "            preds = preds.cpu().numpy().flatten()\n",
    "            y = y.cpu().numpy().flatten()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(y)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(all_targets, label=\"Ground truth\", linewidth=2)\n",
    "    plt.plot(all_preds, label=\"Previsões\", linewidth=2, alpha=0.7)\n",
    "    plt.title(\"Sanity check - Previsões vs Ground Truth (treino)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "sanity_check_plot(model, dataloader, device, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ef72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c436ce4",
   "metadata": {},
   "source": [
    "# Validação do modelo\n",
    "\n",
    "São feitos dois testes:\n",
    "\n",
    "- **Soft test:** Modelo  tenta fazer as previsões, mas não utiliza-as nas previsões futuras, utiliza sempre os *ground truth* como input\n",
    "- **Hard test:** Modelo tenta fazer as previsões, e utiliza $y_{i-1}$ para a previsão de $y_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8561181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model Validation - TO-DO \"\"\"\n",
    "\n",
    "# validate_model()\n",
    "\n",
    "# ===\n",
    "\n",
    "\" Validando o modelo quando o split é menor que 1\"\n",
    "if split < 1:\n",
    "    # Adotando o dataset de validação (soft)\n",
    "    dataset = SingleSeriesDataset(X_test, y_test)\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=False) # shuffle=False para séries temporais\n",
    "\n",
    "    all_preds_S, all_targets_S, avg_loss_test_S = soft_test(model, dataloader, device, criterion)\n",
    "\n",
    "    # Validação hard - previsão cega das primeiras blind_horizon semanas\n",
    "    all_preds_H, all_targets_H, avg_loss_test_H = hard_test(model, X_train, y_train, y_test, split_point, device, criterion, blind_horizon, output_size)\n",
    "\n",
    "    \" Sanity check da validação do modelo (preds and targets)\"\n",
    "    all_preds_array = []\n",
    "    all_targets_array = []\n",
    "\n",
    "    # Convert lists to tensors before flattening\n",
    "    all_preds_tensor = torch.cat([t.unsqueeze(0) if t.dim() == 1 else t for t in all_preds_H], dim=0).flatten()\n",
    "    all_targets_tensor = torch.cat([t.unsqueeze(0) if t.dim() == 1 else t for t in all_targets_H], dim=0).flatten()\n",
    "\n",
    "    for t in all_preds_tensor:\n",
    "        all_preds_array.append(t.detach().numpy())\n",
    "    for t in all_targets_tensor:\n",
    "        all_targets_array.append(t.detach().numpy())\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(all_targets_array, label=\"Ground truth\", linewidth=2)\n",
    "    plt.plot(all_preds_array, label=\"Previsões\", linewidth=2, alpha=0.7)\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab971a0",
   "metadata": {},
   "source": [
    "# Previsão cega (quando split == 1)\n",
    "\n",
    "Previsão para envio para o hackathon das 4 primeiras semanas de janeiro/23. Utiliza todo o dataset como treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Blind prediction \"\"\"\n",
    "\n",
    "blind_prediction = get_blind_prediction(model, X_train, hyperparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cee228b",
   "metadata": {},
   "source": [
    "# Ponto de parada - RAB 18-09-25\n",
    "- Temporais (mais fáceis de pensar e implementar)\n",
    "- Categóricas (intrínsecas do produto ou da loja, como a categoria deles)\n",
    "- De localização (apenas se der tempo)\n",
    "\n",
    "Após termos feito isso, podemos então realizar normalização dos sinais, one hot encoding do que for viável e embedding de IDs etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
